{
 "cells": [
  {
   "cell_type": "raw",
   "id": "d430ca0e-c21d-4ea9-975b-fc1cdd0dfedf",
   "metadata": {},
   "source": [
    "ä»æ­¤æ—¶å¼€å§‹æƒ³agentè¿›å‘ï¼Œcreate_tool_calling_agent + AgentExecutor"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6d6fd1f5-4ff3-4f05-aca0-e8db749630dc",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "18c7d858-4576-4371-a5ea-4e25ef271603",
   "metadata": {},
   "source": [
    "ç»ˆç«¯è¿è¡Œä»£ç ï¼ˆä¸‹åˆ—ï¼‰\n",
    "å¢åŠ äº†max-model-len 16384å’ŒåŠ å…¥äº†gpu-memory-utilization 0.90\n",
    "å¢åŠ äº†agent toolçš„ä¸¤ä¸ª"
   ]
  },
  {
   "cell_type": "raw",
   "id": "414e2e89-b734-4bbe-85bc-58b49c71c3ff",
   "metadata": {},
   "source": [
    "python -m vllm.entrypoints.openai.api_server \\\n",
    "    --model /root/parter/project/models/qwen/Qwen2___5-7B-Instruct \\\n",
    "    --served-model-name Qwen2.5-7B \\\n",
    "    --max-model-len 16384 \\\n",
    "    --gpu-memory-utilization 0.90 \\\n",
    "    --trust-remote-code \\\n",
    "    --port 8000 \\\n",
    "    --enable-auto-tool-choice \\\n",
    "    --tool-call-parser hermes"
   ]
  },
  {
   "cell_type": "raw",
   "id": "aa152534-7e00-4daf-a302-60501e371cb7",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "e5b3cf5e-f65f-4c74-9b5f-08b03e2cbf0e",
   "metadata": {},
   "source": [
    "æ„å»ºRAGæ•°æ®åº“ï¼šåœ¨å¾ªç¯åˆ‡ç‰‡æ—¶ï¼ŒæŠŠæå–åˆ°çš„å¹´ä»½å’Œå…¬å¸å†™å…¥æ•°æ®åº“çš„å…ƒæ•°æ®å­—æ®µï¼ŒåŒæ—¶å†™åœ¨æ–‡æœ¬å¼€å¤´ï¼Œè®© LLM ä¸€çœ¼å°±èƒ½çœ‹åˆ°ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d27c75f-1a16-4efb-b91b-4d00116aee45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import json\n",
    "import re  # ğŸ”¥ğŸ”¥ğŸ”¥ æ–°å¢ï¼šç”¨äºæ­£åˆ™æå–å¹´ä»½\n",
    "\n",
    "# --- 0. ç¯å¢ƒè¡¥ä¸ ---\n",
    "try:\n",
    "    __import__('pysqlite3')\n",
    "    sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "# --- 1. å¯¼å…¥å·¥å…· ---\n",
    "# âœ… ä¿æŒä½¿ç”¨ PDFPlumberLoader (å¤„ç†è¡¨æ ¼æ›´å¥½)\n",
    "from langchain_community.document_loaders import PDFPlumberLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "# ================= é…ç½®åŒº =================\n",
    "DATA_PATH = \"/root/parter/data\"              \n",
    "DB_PATH = \"/root/parter/project/chroma_db\"   \n",
    "RECORD_FILE = \"processed_files.json\"             \n",
    "LOCAL_MODEL_PATH = \"/root/parter/models/AI-ModelScope/bge-small-zh-v1___5\"\n",
    "# =========================================\n",
    "\n",
    "# --- è¾…åŠ©å‡½æ•° ---\n",
    "def load_processed_files():\n",
    "    record_path = os.path.join(DATA_PATH, RECORD_FILE)\n",
    "    if os.path.exists(record_path):\n",
    "        with open(record_path, 'r', encoding='utf-8') as f:\n",
    "            return set(json.load(f))\n",
    "    return set()\n",
    "\n",
    "def save_processed_files(processed_set):\n",
    "    record_path = os.path.join(DATA_PATH, RECORD_FILE)\n",
    "    with open(record_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(list(processed_set), f, ensure_ascii=False, indent=4)\n",
    "\n",
    "# ğŸ”¥ğŸ”¥ğŸ”¥ æ–°å¢æ ¸å¿ƒåŠŸèƒ½ï¼šä»æ–‡ä»¶åæå–å…ƒæ•°æ® ğŸ”¥ğŸ”¥ğŸ”¥\n",
    "def extract_metadata_from_filename(filename):\n",
    "    \"\"\"\n",
    "    è§£ææ–‡ä»¶åï¼Œæå–å…³é”®ä¿¡æ¯ã€‚\n",
    "    ä¾‹å¦‚: 'è´µå·èŒ…å°_2023å¹´æŠ¥.pdf' -> {'year': '2023', 'company': 'è´µå·èŒ…å°'}\n",
    "    \"\"\"\n",
    "    metadata = {\"source\": filename}\n",
    "    \n",
    "    # 1. æå–å¹´ä»½ (åŒ¹é…è¿ç»­4ä½æ•°å­—)\n",
    "    year_match = re.search(r'(\\d{4})', filename)\n",
    "    if year_match:\n",
    "        metadata['year'] = year_match.group(1)\n",
    "    else:\n",
    "        metadata['year'] = \"æœªçŸ¥å¹´ä»½\"\n",
    "    \n",
    "    # 2. æå–å…¬å¸å (ç­–ç•¥ï¼šå»æ‰.pdf -> å»æ‰å¹´ä»½ -> å»æ‰å¸¸è§åç¼€)\n",
    "    clean_name = filename.replace('.pdf', '')\n",
    "    if year_match:\n",
    "        clean_name = clean_name.replace(year_match.group(1), '')\n",
    "    \n",
    "    # å»é™¤æ‚è´¨ (ä½ å¯ä»¥æ ¹æ®ä½ çš„æ–‡ä»¶åä¹ æƒ¯åœ¨è¿™é‡Œæ·»åŠ æ›´å¤šè¿‡æ»¤è¯)\n",
    "    clean_name = clean_name.replace('å¹´æŠ¥', '').replace('å¹´åº¦æŠ¥å‘Š', '').replace('_', '').replace(' ', '').strip()\n",
    "    \n",
    "    if not clean_name:\n",
    "        clean_name = \"æœªçŸ¥å…¬å¸\"\n",
    "        \n",
    "    metadata['company'] = clean_name\n",
    "    return metadata\n",
    "\n",
    "# --- ä¸»ç¨‹åº ---\n",
    "print(f\"ğŸš€ å¯åŠ¨ã€Agentå¢å¼ºç‰ˆ - PDFPlumberã€‘å…¥åº“ç¨‹åº...\")\n",
    "\n",
    "\n",
    "if not os.path.exists(DATA_PATH):\n",
    "    print(f\"âŒ é”™è¯¯ï¼šæ‰¾ä¸åˆ°æ•°æ®æ–‡ä»¶å¤¹ {DATA_PATH}\")\n",
    "    sys.exit()\n",
    "\n",
    "all_files = [f for f in os.listdir(DATA_PATH) if f.lower().endswith('.pdf')]\n",
    "processed_files = load_processed_files()\n",
    "new_files = [f for f in all_files if f not in processed_files]\n",
    "\n",
    "if not new_files:\n",
    "    print(\"âœ… æ²¡æœ‰æ–°æ–‡ä»¶ï¼Œæ— éœ€æ›´æ–°ã€‚\")\n",
    "else:\n",
    "    print(f\"ğŸ” å‘ç° {len(new_files)} ä¸ªæ–°æ–‡ä»¶ï¼Œå‡†å¤‡è¿›è¡Œå…ƒæ•°æ®å¢å¼ºè§£æ...\")\n",
    "\n",
    "    # åŠ è½½æ¨¡å‹\n",
    "    if not os.path.exists(LOCAL_MODEL_PATH):\n",
    "        LOCAL_MODEL_PATH = \"/root/parter/models/bge-small-zh-v1.5\"\n",
    "     \n",
    "    print(f\"ğŸ§  åŠ è½½ Embedding æ¨¡å‹ (CPUæ¨¡å¼)...\")\n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name=LOCAL_MODEL_PATH,\n",
    "        model_kwargs={'device': 'cpu'} \n",
    "    )\n",
    "\n",
    "    # è¿æ¥æ•°æ®åº“\n",
    "    vectorstore = Chroma(persist_directory=DB_PATH, embedding_function=embeddings)\n",
    "\n",
    "    # âœ… ä¿æŒä½ è®¾å®šçš„ chunk_size=2000\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=2000, \n",
    "        chunk_overlap=200,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \"ã€‚\", \"ï¼›\", \" \", \"\"]\n",
    "    )\n",
    "    \n",
    "    success_count = 0\n",
    "\n",
    "    for filename in new_files:\n",
    "        file_path = os.path.join(DATA_PATH, filename)\n",
    "        print(f\"â³ æ­£åœ¨æ·±åº¦è§£æ (PDFPlumber): {filename} ...\")\n",
    "        \n",
    "        try:\n",
    "            # 1. ğŸ”¥ å…ˆæå–å…ƒæ•°æ®\n",
    "            file_meta = extract_metadata_from_filename(filename)\n",
    "            print(f\"   -> è¯†åˆ«ä¿¡æ¯: å…¬å¸[{file_meta['company']}] | å¹´ä»½[{file_meta['year']}]\")\n",
    "\n",
    "            # 2. åŠ è½½æ–‡æ¡£\n",
    "            loader = PDFPlumberLoader(file_path)\n",
    "            docs = loader.load()\n",
    "            \n",
    "            # 3. åˆ‡åˆ†æ–‡æ¡£\n",
    "            splits = text_splitter.split_documents(docs)\n",
    "            \n",
    "            # 4. ğŸ”¥ğŸ”¥ğŸ”¥ æ³¨å…¥å…ƒæ•°æ® (å…³é”®æ­¥éª¤) ğŸ”¥ğŸ”¥ğŸ”¥\n",
    "            for split in splits:\n",
    "                # æ›´æ–° Chroma çš„ metadata å­—æ®µ (ç»™ Agent æ£€ç´¢ç”¨)\n",
    "                split.metadata.update(file_meta)\n",
    "                \n",
    "                # æ›´æ–° page_content (ç»™ LLM é˜…è¯»ç”¨ï¼Œè®©å®ƒçŸ¥é“è¿™æ®µè¯å±äºè°)\n",
    "                split.page_content = f\"ã€å…¬å¸:{file_meta['company']} | å¹´ä»½:{file_meta['year']} | æ¥æº:{filename}ã€‘\\n{split.page_content}\"\n",
    "\n",
    "            if splits:\n",
    "                vectorstore.add_documents(splits)\n",
    "                print(f\"   -> âœ… å…¥åº“æˆåŠŸ: {len(splits)} ç‰‡æ®µ\")\n",
    "                processed_files.add(filename)\n",
    "                success_count += 1\n",
    "            else:\n",
    "                print(\"   -> âš ï¸ æ–‡ä»¶è§£æåå†…å®¹ä¸ºç©º\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"   -> âŒ å¤±è´¥: {e}\")\n",
    "\n",
    "    if success_count > 0:\n",
    "        save_processed_files(processed_files)\n",
    "        print(\"\\nğŸ‰ å…¨éƒ¨å¤„ç†å®Œæˆï¼æ•°æ®åº“å·²åŒ…å«å…ƒæ•°æ®ï¼ŒAgent ç°åœ¨å¯ä»¥ç²¾å‡†æŸ¥æ•°äº†ã€‚\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ca976a7b-dd93-4e0b-9873-ef79c59f3757",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "5fec5e26-60cd-4cb6-81ee-c4edec34ece7",
   "metadata": {},
   "source": [
    "åŸå› ï¼šå› ä¸ºaiæ¨¡å‹å¹»æƒ³çš„åŸå› ï¼ŒæŸ¥è¯¢è‚¡ç¥¨åç§°ä»·æ ¼ä¸å‡†ç¡®\n",
    "ç»“æœï¼šå°†è‚¡ç¥¨åç§°å’Œä»£ç ç»‘å®šåœ¨ä¸€èµ·ï¼Œä¸‹è¿°æ˜¯æ›´æ–°è‚¡ç¥¨ä»£ç çš„ä»£ç \n",
    "\n",
    "å…¶å®æ˜¯æ¸…æ¥šå†å²è®°å¿†åæ‰å®Œå…¨è§£å†³è¯¥é—®é¢˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3cace164-0b01-44b9-b81c-d63f184ac083",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”„ [æ‰‹åŠ¨æ›´æ–°] æ­£åœ¨è¿æ¥ AkShare ä¸‹è½½å…¨å¸‚åœºæ•°æ®...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/57 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… æ›´æ–°æˆåŠŸï¼å·²ä¿å­˜ 5793 æ¡æ•°æ®åˆ° parter/data/stock_cache.pkl\n"
     ]
    }
   ],
   "source": [
    "# === å•å…ƒæ ¼ Aï¼šæ‰‹åŠ¨æ›´æ–°è‚¡ç¥¨ä»£ç åº“ ===\n",
    "import pandas as pd\n",
    "import akshare as ak\n",
    "\n",
    "CACHE_FILE = \"/root/parter/data/stock_cache.pkl\"  # ç¡®ä¿æ–‡ä»¶åä¸ä¸»ç¨‹åºä¸€è‡´\n",
    "\n",
    "def update_stock_db():\n",
    "    print(\"ğŸ”„ [æ‰‹åŠ¨æ›´æ–°] æ­£åœ¨è¿æ¥ AkShare ä¸‹è½½å…¨å¸‚åœºæ•°æ®...\")\n",
    "    try:\n",
    "        # ä¸‹è½½ä»£ç å’Œåç§°\n",
    "        df = ak.stock_zh_a_spot_em()[['ä»£ç ', 'åç§°']]\n",
    "        # å¼ºåˆ¶è½¬æ¢ä¸ºå­—ç¬¦ä¸²\n",
    "        df['ä»£ç '] = df['ä»£ç '].astype(str)\n",
    "        # ä¿å­˜\n",
    "        df.to_pickle(CACHE_FILE)\n",
    "        print(f\"âœ… æ›´æ–°æˆåŠŸï¼å·²ä¿å­˜ {len(df)} æ¡æ•°æ®åˆ° {CACHE_FILE}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ æ›´æ–°å¤±è´¥: {e}\")\n",
    "\n",
    "#ä»¥æ­¤è¡Œå‘½ä»¤è§¦å‘æ›´æ–°\n",
    "update_stock_db()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "adca6e3e-01b3-4956-b3d5-0f8901e35ed5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e25cef4-2091-492c-b815-84f325d8e727",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ æ­£åœ¨åˆå§‹åŒ–æ™ºèƒ½é‡‘è Agent (RAG + Tools)...\n",
      "âœ… å·²è¿æ¥æ¨¡å‹: Qwen2.5-7B\n",
      "âœ… æœ¬åœ°è‚¡ç¥¨åº“åŠ è½½æˆåŠŸ (å…± 5793 æ¡)\n",
      "âš™ï¸ æ„å»ºæ£€ç´¢æµæ°´çº¿ (BM25 + Vector + Rerank)...\n",
      "âœ… Reranker åŠ è½½æˆåŠŸã€‚\n",
      "\n",
      "==================================================\n",
      "ğŸ¤– æŠ•ç ” Agent å¯åŠ¨å®Œæ¯•ï¼(è¾“å…¥ quit é€€å‡º)\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ‘‰ è¯·æé—®:  æ°¸è¾‰è¶…å¸‚è‚¡ç¥¨ä»·æ ¼\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `get_stock_price` with `{'ticker': 'æ°¸è¾‰è¶…å¸‚'}`\n",
      "\n",
      "\n",
      "\u001b[0m\n",
      "ğŸ“ˆ [è¡Œæƒ…å·¥å…·] ç”¨æˆ·è¾“å…¥: æ°¸è¾‰è¶…å¸‚ ...\n",
      "   -> è¯†åˆ«ä¸ºåç§°ï¼Œæ­£åœ¨æœ¬åœ°åº“æŸ¥æ‰¾...\n",
      "   -> âœ… åŒ¹é…æˆåŠŸ: æ°¸è¾‰è¶…å¸‚ -> 601933\n",
      "\u001b[33;1m\u001b[1;3mã€æ–°æµªå®æ—¶ã€‘æ°¸è¾‰è¶…å¸‚ (601933) å½“å‰ä»·æ ¼: 4.960 å…ƒ | æ—¶é—´: 2026-01-06 15:00:00\u001b[0m\u001b[32;1m\u001b[1;3mæˆªè‡³2026å¹´01æœˆ06æ—¥15:00ï¼Œæ°¸è¾‰è¶…å¸‚çš„è‚¡ç¥¨ä»·æ ¼ä¸º4.960å…ƒã€‚\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "ğŸ¤– å›ç­”: æˆªè‡³2026å¹´01æœˆ06æ—¥15:00ï¼Œæ°¸è¾‰è¶…å¸‚çš„è‚¡ç¥¨ä»·æ ¼ä¸º4.960å…ƒã€‚\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ‘‰ è¯·æé—®:  åèƒœå¤©æˆè‚¡ç¥¨ä»·æ ¼\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `get_stock_price` with `{'ticker': 'åèƒœå¤©æˆ'}`\n",
      "\n",
      "\n",
      "\u001b[0m\n",
      "ğŸ“ˆ [è¡Œæƒ…å·¥å…·] ç”¨æˆ·è¾“å…¥: åèƒœå¤©æˆ ...\n",
      "   -> è¯†åˆ«ä¸ºåç§°ï¼Œæ­£åœ¨æœ¬åœ°åº“æŸ¥æ‰¾...\n",
      "   -> âœ… åŒ¹é…æˆåŠŸ: åèƒœå¤©æˆ -> 600410\n",
      "\u001b[33;1m\u001b[1;3mã€æ–°æµªå®æ—¶ã€‘åèƒœå¤©æˆ (600410) å½“å‰ä»·æ ¼: 18.450 å…ƒ | æ—¶é—´: 2026-01-06 15:00:01\u001b[0m\u001b[32;1m\u001b[1;3måèƒœå¤©æˆï¼ˆè‚¡ç¥¨ä»£ç ï¼š600410ï¼‰çš„å½“å‰ä»·æ ¼ä¸º18.450å…ƒã€‚æ•°æ®æ¥æºäºæ–°æµªå®æ—¶æŠ¥ä»·ï¼Œæ—¶é—´ä¸º2026å¹´01æœˆ06æ—¥15:00:01ã€‚\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "ğŸ¤– å›ç­”: åèƒœå¤©æˆï¼ˆè‚¡ç¥¨ä»£ç ï¼š600410ï¼‰çš„å½“å‰ä»·æ ¼ä¸º18.450å…ƒã€‚æ•°æ®æ¥æºäºæ–°æµªå®æ—¶æŠ¥ä»·ï¼Œæ—¶é—´ä¸º2026å¹´01æœˆ06æ—¥15:00:01ã€‚\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ‘‰ è¯·æé—®:  å·¥ä¸šå¯Œè”è‚¡ç¥¨ä»·æ ¼\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `get_stock_price` with `{'ticker': 'å·¥ä¸šå¯Œè”'}`\n",
      "\n",
      "\n",
      "\u001b[0m\n",
      "ğŸ“ˆ [è¡Œæƒ…å·¥å…·] ç”¨æˆ·è¾“å…¥: å·¥ä¸šå¯Œè” ...\n",
      "   -> è¯†åˆ«ä¸ºåç§°ï¼Œæ­£åœ¨æœ¬åœ°åº“æŸ¥æ‰¾...\n",
      "   -> âœ… åŒ¹é…æˆåŠŸ: å·¥ä¸šå¯Œè” -> 601138\n",
      "\u001b[33;1m\u001b[1;3mã€æ–°æµªå®æ—¶ã€‘å·¥ä¸šå¯Œè” (601138) å½“å‰ä»·æ ¼: 64.050 å…ƒ | æ—¶é—´: 2026-01-06 15:00:03\u001b[0m\u001b[32;1m\u001b[1;3må·¥ä¸šå¯Œè”ï¼ˆè‚¡ç¥¨ä»£ç ï¼š601138ï¼‰å½“å‰çš„ä»·æ ¼æ˜¯64.050å…ƒã€‚è¯·æ³¨æ„ï¼Œè¿™æ˜¯æˆªè‡³2026å¹´01æœˆ06æ—¥15:00:03çš„å®æ—¶ä»·æ ¼ã€‚\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "ğŸ¤– å›ç­”: å·¥ä¸šå¯Œè”ï¼ˆè‚¡ç¥¨ä»£ç ï¼š601138ï¼‰å½“å‰çš„ä»·æ ¼æ˜¯64.050å…ƒã€‚è¯·æ³¨æ„ï¼Œè¿™æ˜¯æˆªè‡³2026å¹´01æœˆ06æ—¥15:00:03çš„å®æ—¶ä»·æ ¼ã€‚\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ‘‰ è¯·æé—®:  æœ‰å…³æ°¸è¾‰è¶…å¸‚æœ€è¿‘çš„æ–°é—»\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `get_latest_news` with `{'symbol': '601933'}`\n",
      "\n",
      "\n",
      "\u001b[0m\n",
      "ğŸ“° [å¿«è®¯å·¥å…·] æ­£åœ¨è·å–: 601933 ...\n",
      "\u001b[33;1m\u001b[1;3mã€601933 æœ€æ–°ä¸ªè‚¡èµ„è®¯ã€‘\n",
      "- [2025-12-10 11:23:02] 600693ã€601933è¿ç»­æ¶¨åœï¼è‚¡ä¸œé«˜ä½å‡æŒ\n",
      "- [2025-12-10 12:37:45] å°å•è¶…400ä¸‡æ‰‹ 601933ä¸‰è¿æ¿ è¿™ä¸ªæ¿å—æ€æ¶¨åœæ½®ï¼\n",
      "- [2025-12-29 19:44:04] æ°¸è¾‰è¶…å¸‚æ‹Ÿæ¸…ä»“æ‰€æŒäº‘é‡‘ç§‘æŠ€28.095%è‚¡æƒï¼ŒæŒ‚ç‰Œåº•ä»·1.78äº¿å…ƒ\n",
      "- [2025-12-29 18:11:15] æ°¸è¾‰è¶…å¸‚ï¼šæ‹Ÿå…¬å¼€æŒ‚ç‰Œå‡ºå”®å‚è‚¡å­å…¬å¸æ°¸è¾‰äº‘é‡‘ç§‘æŠ€28.095%è‚¡æƒ\n",
      "- [2025-12-29 18:11:15] æ°¸è¾‰è¶…å¸‚ï¼šæ‹Ÿå…¬å¼€æŒ‚ç‰Œè½¬è®©äº‘é‡‘ç§‘æŠ€28.095%è‚¡æƒ\n",
      "\u001b[0m\u001b[32;1m\u001b[1;3mæ ¹æ®è´¢è”ç¤¾å¿«è®¯ï¼Œæ°¸è¾‰è¶…å¸‚è¿‘æœŸçš„ä¸€äº›é‡è¦æ–°é—»å¦‚ä¸‹ï¼š\n",
      "\n",
      "- 2025å¹´12æœˆ10æ—¥ï¼Œ600693å’Œ601933è¿ç»­æ¶¨åœï¼ŒåŒæ—¶æœ‰è‚¡ä¸œåœ¨é«˜ä½å‡æŒã€‚\n",
      "- 2025å¹´12æœˆ10æ—¥ï¼Œå°å•è¶…è¿‡400ä¸‡æ‰‹ï¼Œ601933å‡ºç°ä¸‰è¿æ¿ç°è±¡ï¼Œå¸¦åŠ¨æ•´ä¸ªæ¿å—æ¶¨åœæ½®ã€‚\n",
      "- 2025å¹´12æœˆ29æ—¥ï¼Œæ°¸è¾‰è¶…å¸‚è®¡åˆ’æ¸…ä»“å…¶æ‰€æŒæœ‰çš„äº‘é‡‘ç§‘æŠ€28.095%çš„è‚¡æƒï¼ŒæŒ‚ç‰Œåº•ä»·ä¸º1.78äº¿å…ƒã€‚\n",
      "- åŒä¸€å¤©ï¼Œæ°¸è¾‰è¶…å¸‚å®£å¸ƒæ‹Ÿé€šè¿‡å…¬å¼€æŒ‚ç‰Œçš„æ–¹å¼å‡ºå”®å…¶å‚è‚¡å­å…¬å¸æ°¸è¾‰äº‘é‡‘ç§‘æŠ€28.095%çš„è‚¡æƒã€‚\n",
      "- åŒä¸€å¤©ï¼Œæ°¸è¾‰è¶…å¸‚è¿˜å‘å¸ƒäº†å…³äºæ‹Ÿå…¬å¼€æŒ‚ç‰Œè½¬è®©äº‘é‡‘ç§‘æŠ€28.095%è‚¡æƒçš„ä¿¡æ¯ã€‚\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "ğŸ¤– å›ç­”: æ ¹æ®è´¢è”ç¤¾å¿«è®¯ï¼Œæ°¸è¾‰è¶…å¸‚è¿‘æœŸçš„ä¸€äº›é‡è¦æ–°é—»å¦‚ä¸‹ï¼š\n",
      "\n",
      "- 2025å¹´12æœˆ10æ—¥ï¼Œ600693å’Œ601933è¿ç»­æ¶¨åœï¼ŒåŒæ—¶æœ‰è‚¡ä¸œåœ¨é«˜ä½å‡æŒã€‚\n",
      "- 2025å¹´12æœˆ10æ—¥ï¼Œå°å•è¶…è¿‡400ä¸‡æ‰‹ï¼Œ601933å‡ºç°ä¸‰è¿æ¿ç°è±¡ï¼Œå¸¦åŠ¨æ•´ä¸ªæ¿å—æ¶¨åœæ½®ã€‚\n",
      "- 2025å¹´12æœˆ29æ—¥ï¼Œæ°¸è¾‰è¶…å¸‚è®¡åˆ’æ¸…ä»“å…¶æ‰€æŒæœ‰çš„äº‘é‡‘ç§‘æŠ€28.095%çš„è‚¡æƒï¼ŒæŒ‚ç‰Œåº•ä»·ä¸º1.78äº¿å…ƒã€‚\n",
      "- åŒä¸€å¤©ï¼Œæ°¸è¾‰è¶…å¸‚å®£å¸ƒæ‹Ÿé€šè¿‡å…¬å¼€æŒ‚ç‰Œçš„æ–¹å¼å‡ºå”®å…¶å‚è‚¡å­å…¬å¸æ°¸è¾‰äº‘é‡‘ç§‘æŠ€28.095%çš„è‚¡æƒã€‚\n",
      "- åŒä¸€å¤©ï¼Œæ°¸è¾‰è¶…å¸‚è¿˜å‘å¸ƒäº†å…³äºæ‹Ÿå…¬å¼€æŒ‚ç‰Œè½¬è®©äº‘é‡‘ç§‘æŠ€28.095%è‚¡æƒçš„ä¿¡æ¯ã€‚\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ‘‰ è¯·æé—®:  å·¥ä¸šå¯Œè”è‚¡ç¥¨æœ€è¿‘æ€ä¹ˆæ ·ï¼Ÿ\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `get_latest_news` with `{'symbol': '601138'}`\n",
      "\n",
      "\n",
      "\u001b[0m\n",
      "ğŸ“° [å¿«è®¯å·¥å…·] æ­£åœ¨è·å–: 601138 ...\n",
      "\u001b[33;1m\u001b[1;3mã€601138 æœ€æ–°ä¸ªè‚¡èµ„è®¯ã€‘\n",
      "- [2026-01-04 16:31:10] å·¥ä¸šå¯Œè”ï¼šå·²ç´¯è®¡å›è´­931.99ä¸‡è‚¡è‚¡ä»½\n",
      "- [2026-01-05 10:49:21] ä¸Šè¯180ETFæŒ‡æ•°åŸºé‡‘(530280æ¶¨è¶…1.1%ï¼Œæœºæ„ç§°æ˜¥å­£èºåŠ¨è¡Œæƒ…å¯èƒ½æå‰å¼€å¯\n",
      "- [2026-01-05 13:35:27] æ¶ˆè´¹ç”µå­ETF(561600æ¶¨è¶…2.7%ï¼Œæ”¿ç­–å¼•å¯¼ç”µå™¨ç”µå­äº§å“ã€æ±½è½¦ã€åŠ¨åŠ›ç”µæ± ç­‰ç”Ÿäº§ä¼ä¸šå‚ä¸å›æ”¶åˆ©ç”¨\n",
      "- [2026-01-05 17:17:42] ç”µå­è¡Œä¸š1æœˆ5æ—¥èµ„é‡‘æµå‘æ—¥æŠ¥\n",
      "- [2025-12-31 10:08:12] AIç»ˆç«¯åˆ›æ–°åœ¨2026å¹´æœ‰æœ›è¿æ¥å…ƒå¹´ï¼Œæ¶ˆè´¹ç”µå­ETF(561600å¤‡å—å…³æ³¨\n",
      "\u001b[0m\u001b[32;1m\u001b[1;3mæ ¹æ®æœ€æ–°è´¢ç»å¿«è®¯ï¼Œå·¥ä¸šå¯Œè”ï¼ˆ601138ï¼‰è¿‘æœŸæœ‰ä¸€äº›é‡è¦åŠ¨æ€ï¼š\n",
      "\n",
      "- 2026å¹´1æœˆ4æ—¥ï¼Œå·¥ä¸šå¯Œè”å®£å¸ƒå·²ç´¯è®¡å›è´­931.99ä¸‡è‚¡è‚¡ä»½ã€‚\n",
      "- 2026å¹´1æœˆ5æ—¥ï¼Œä¸Šè¯180ETFæŒ‡æ•°åŸºé‡‘ä¸Šæ¶¨è¶…è¿‡1.1%ï¼Œæœ‰æœºæ„è®¤ä¸ºæ˜¥å­£èºåŠ¨è¡Œæƒ…å¯èƒ½æå‰å¼€å¯ã€‚\n",
      "- åŒä¸€å¤©ï¼Œæ¶ˆè´¹ç”µå­ETFä¹Ÿä¸Šæ¶¨è¶…è¿‡2.7%ï¼Œè¿™å¯èƒ½ä¸æ”¿ç­–å¯¹ç”µå™¨ç”µå­äº§å“ã€æ±½è½¦ã€åŠ¨åŠ›ç”µæ± ç­‰ç”Ÿäº§ä¼ä¸šçš„å›æ”¶åˆ©ç”¨æ”¯æŒæœ‰å…³ã€‚\n",
      "- ç”µå­è¡Œä¸š1æœˆ5æ—¥çš„èµ„é‡‘æµå‘æ—¥æŠ¥æ˜¾ç¤ºäº†å¸‚åœºçš„å…³æ³¨ç‚¹ã€‚\n",
      "- æ­¤å¤–ï¼ŒAIç»ˆç«¯åˆ›æ–°é¢„è®¡åœ¨2026å¹´å°†è¿æ¥å…ƒå¹´ï¼Œè¿™ä¹Ÿä½¿å¾—æ¶ˆè´¹ç”µå­ETFå¤‡å—å…³æ³¨ã€‚\n",
      "\n",
      "è¿™äº›ä¿¡æ¯è¡¨æ˜å·¥ä¸šå¯Œè”åŠå…¶ç›¸å…³è¡Œä¸šè¿‘æœŸè¡¨ç°æ´»è·ƒï¼Œä½†å…·ä½“è‚¡ä»·å˜åŠ¨è¿˜éœ€è¿›ä¸€æ­¥æŸ¥çœ‹å®æ—¶è¡Œæƒ…ã€‚\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "ğŸ¤– å›ç­”: æ ¹æ®æœ€æ–°è´¢ç»å¿«è®¯ï¼Œå·¥ä¸šå¯Œè”ï¼ˆ601138ï¼‰è¿‘æœŸæœ‰ä¸€äº›é‡è¦åŠ¨æ€ï¼š\n",
      "\n",
      "- 2026å¹´1æœˆ4æ—¥ï¼Œå·¥ä¸šå¯Œè”å®£å¸ƒå·²ç´¯è®¡å›è´­931.99ä¸‡è‚¡è‚¡ä»½ã€‚\n",
      "- 2026å¹´1æœˆ5æ—¥ï¼Œä¸Šè¯180ETFæŒ‡æ•°åŸºé‡‘ä¸Šæ¶¨è¶…è¿‡1.1%ï¼Œæœ‰æœºæ„è®¤ä¸ºæ˜¥å­£èºåŠ¨è¡Œæƒ…å¯èƒ½æå‰å¼€å¯ã€‚\n",
      "- åŒä¸€å¤©ï¼Œæ¶ˆè´¹ç”µå­ETFä¹Ÿä¸Šæ¶¨è¶…è¿‡2.7%ï¼Œè¿™å¯èƒ½ä¸æ”¿ç­–å¯¹ç”µå™¨ç”µå­äº§å“ã€æ±½è½¦ã€åŠ¨åŠ›ç”µæ± ç­‰ç”Ÿäº§ä¼ä¸šçš„å›æ”¶åˆ©ç”¨æ”¯æŒæœ‰å…³ã€‚\n",
      "- ç”µå­è¡Œä¸š1æœˆ5æ—¥çš„èµ„é‡‘æµå‘æ—¥æŠ¥æ˜¾ç¤ºäº†å¸‚åœºçš„å…³æ³¨ç‚¹ã€‚\n",
      "- æ­¤å¤–ï¼ŒAIç»ˆç«¯åˆ›æ–°é¢„è®¡åœ¨2026å¹´å°†è¿æ¥å…ƒå¹´ï¼Œè¿™ä¹Ÿä½¿å¾—æ¶ˆè´¹ç”µå­ETFå¤‡å—å…³æ³¨ã€‚\n",
      "\n",
      "è¿™äº›ä¿¡æ¯è¡¨æ˜å·¥ä¸šå¯Œè”åŠå…¶ç›¸å…³è¡Œä¸šè¿‘æœŸè¡¨ç°æ´»è·ƒï¼Œä½†å…·ä½“è‚¡ä»·å˜åŠ¨è¿˜éœ€è¿›ä¸€æ­¥æŸ¥çœ‹å®æ—¶è¡Œæƒ…ã€‚\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import requests\n",
    "import json\n",
    "import math\n",
    "import random\n",
    "\n",
    "import yfinance as yf   #é›…è™è´¢ç»\n",
    "import datetime         #æ—¶é—´\n",
    "import akshare as ak    #å¼•å…¥ AkShare è·å–å®æ—¶èµ„è®¯\n",
    "import pandas as pd     #éœ€è¦ä¸€ä¸ªæ–°çš„è‚¡ç¥¨åç§°ä»£ç è¡¨\n",
    "\n",
    "# --- 0. ç¯å¢ƒè¡¥ä¸ (é˜²æ­¢ sqlite3 ç‰ˆæœ¬æŠ¥é”™) ---\n",
    "try:\n",
    "    __import__('pysqlite3')\n",
    "    sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "# --- 1. å¯¼å…¥å·¥å…· ---\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.agents import create_tool_calling_agent, AgentExecutor\n",
    "from langchain_core.tools import tool\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.docstore.document import Document\n",
    "from langchain_community.tools import DuckDuckGoSearchRun   #DuckDuckGo Searchè”ç½‘æŸ¥è¯¢å·¥å…·\n",
    "# é«˜çº§æ£€ç´¢ç»„ä»¶\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "from langchain.retrievers import EnsembleRetriever, ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import CrossEncoderReranker\n",
    "from langchain_community.cross_encoders import HuggingFaceCrossEncoder\n",
    "\n",
    "# ================= é…ç½®åŒº =================\n",
    "DB_PATH = \"/root/parter/project/chroma_db\"\n",
    "# Embedding æ¨¡å‹ (CPU)\n",
    "EMBEDDING_MODEL_PATH = \"/root/parter/models/AI-ModelScope/bge-small-zh-v1___5\"\n",
    "if not os.path.exists(EMBEDDING_MODEL_PATH):\n",
    "    EMBEDDING_MODEL_PATH = \"/root/parter/models/bge-small-zh-v1.5\"\n",
    "\n",
    "# Reranker æ¨¡å‹ (CPU)\n",
    "RERANKER_MODEL_PATH = \"/root/parter/models/bge-reranker-base\" \n",
    "\n",
    "# æœ¬åœ° LLM æ¥å£\n",
    "VLLM_API_URL = \"http://localhost:8000/v1\"\n",
    "# =========================================\n",
    "\n",
    "print(\"ğŸš€ æ­£åœ¨åˆå§‹åŒ–æ™ºèƒ½é‡‘è Agent (RAG + Tools)...\")\n",
    "\n",
    "# --- 2. å‡†å¤‡æ¨¡å‹ ---\n",
    "# è‡ªåŠ¨è·å–çœŸå®æ¨¡å‹åç§°\n",
    "try:\n",
    "    resp = requests.get(f\"{VLLM_API_URL}/models\")\n",
    "    if resp.status_code == 200:\n",
    "        REAL_MODEL_NAME = resp.json()['data'][0]['id']\n",
    "        print(f\"âœ… å·²è¿æ¥æ¨¡å‹: {REAL_MODEL_NAME}\")\n",
    "    else:\n",
    "        raise Exception\n",
    "except:\n",
    "    print(\"âš ï¸ æ— æ³•è¿æ¥ vLLMï¼Œä½¿ç”¨é»˜è®¤åç§° fallback\")\n",
    "    REAL_MODEL_NAME = \"gpt-3.5-turbo\"\n",
    "\n",
    "# Embedding (å¼ºåˆ¶ CPU)\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=EMBEDDING_MODEL_PATH, \n",
    "    model_kwargs={'device': 'cpu'}\n",
    ")\n",
    "\n",
    "ddg_search = DuckDuckGoSearchRun()  # è”ç½‘åˆå§‹åŒ–å·¥å…·\n",
    "\n",
    "def get_current_date():\n",
    "    # æ­£å¸¸ç”¨æ³•ï¼š\n",
    "    return datetime.datetime.now().strftime(\"%Yå¹´%mæœˆ%dæ—¥\")\n",
    "\n",
    "#è‚¡ç¥¨åç§°ä»£ç è¡¨æ ¼åŠ è½½\n",
    "CACHE_FILE = \"/root/parter/data/stock_cache.pkl\" # å¿…é¡»ä¸å•å…ƒæ ¼Açš„æ–‡ä»¶åä¸€è‡´\n",
    "GLOBAL_STOCK_DF = None         # ç”¨äºå­˜æ”¾åŠ è½½åçš„æ•°æ®\n",
    "def load_stock_db():\n",
    "    \"\"\"åªè¯»å–æœ¬åœ°æ–‡ä»¶ï¼Œä¸è”ç½‘\"\"\"\n",
    "    global GLOBAL_STOCK_DF\n",
    "    if not os.path.exists(CACHE_FILE):\n",
    "        print(f\"âš ï¸ æœªæ‰¾åˆ°ç¼“å­˜æ–‡ä»¶ {CACHE_FILE}ï¼Œæ— æ³•ä½¿ç”¨åç§°æŸ¥è¯¢åŠŸèƒ½ã€‚\")\n",
    "        print(\"ğŸ‘‰ è¯·å…ˆè¿è¡Œ'å•å…ƒæ ¼A'è¿›è¡Œæ•°æ®åˆå§‹åŒ–ã€‚\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        GLOBAL_STOCK_DF = pd.read_pickle(CACHE_FILE)\n",
    "        print(f\"âœ… æœ¬åœ°è‚¡ç¥¨åº“åŠ è½½æˆåŠŸ (å…± {len(GLOBAL_STOCK_DF)} æ¡)\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ è¯»å–ç¼“å­˜å¤±è´¥: {e}\")\n",
    "\n",
    "load_stock_db()\n",
    "\n",
    "# LLM (ç”¨äº Agent æ¨ç†)\n",
    "llm = ChatOpenAI(\n",
    "    model=REAL_MODEL_NAME,\n",
    "    openai_api_base=VLLM_API_URL,\n",
    "    openai_api_key=\"EMPTY\",\n",
    "    temperature=0.1, # ä½æ¸©åº¦ï¼Œä¿è¯å·¥å…·è°ƒç”¨å‡†ç¡®\n",
    "    max_tokens=2048\n",
    ")\n",
    "\n",
    "# --- 3. å‡†å¤‡ RAG æ£€ç´¢æµæ°´çº¿ (ä½œä¸ºå·¥å…·å†…æ ¸) ---\n",
    "print(\"âš™ï¸ æ„å»ºæ£€ç´¢æµæ°´çº¿ (BM25 + Vector + Rerank)...\")\n",
    "\n",
    "if not os.path.exists(DB_PATH):\n",
    "    print(f\"âŒ ä¸¥é‡é”™è¯¯: æ•°æ®åº“è·¯å¾„ {DB_PATH} ä¸å­˜åœ¨ï¼è¯·å…ˆè¿è¡Œå…¥åº“ä»£ç ã€‚\")\n",
    "    sys.exit()\n",
    "\n",
    "vectorstore = Chroma(persist_directory=DB_PATH, embedding_function=embeddings)\n",
    "\n",
    "# 3.1 æ„å»º BM25 (å…³é”®è¯æ£€ç´¢)\n",
    "all_data = vectorstore.get()\n",
    "docs_for_bm25 = [Document(page_content=t, metadata=m) for t, m in zip(all_data['documents'], all_data['metadatas'])]\n",
    "if not docs_for_bm25:\n",
    "    print(\"âŒ æ•°æ®åº“ä¸ºç©ºï¼è¯·å…ˆè¿è¡Œå…¥åº“ä»£ç ã€‚\")\n",
    "    sys.exit()\n",
    "\n",
    "bm25_retriever = BM25Retriever.from_documents(docs_for_bm25)\n",
    "bm25_retriever.k = 10\n",
    "\n",
    "# 3.2 å‘é‡æ£€ç´¢\n",
    "vector_retriever = vectorstore.as_retriever(search_kwargs={\"k\": 20})\n",
    "\n",
    "# 3.3 æ··åˆæ£€ç´¢\n",
    "ensemble_retriever = EnsembleRetriever(\n",
    "    retrievers=[bm25_retriever, vector_retriever],\n",
    "    weights=[0.4, 0.6]\n",
    ")\n",
    "\n",
    "# 3.4 é‡æ’åº (Reranker - CPU)\n",
    "try:\n",
    "    rerank_model = HuggingFaceCrossEncoder(\n",
    "        model_name=RERANKER_MODEL_PATH, \n",
    "        model_kwargs={'device': 'cpu'}\n",
    "    )\n",
    "    compressor = CrossEncoderReranker(model=rerank_model, top_n=5)\n",
    "    final_retriever = ContextualCompressionRetriever(\n",
    "        base_compressor=compressor, \n",
    "        base_retriever=ensemble_retriever\n",
    "    )\n",
    "    print(\"âœ… Reranker åŠ è½½æˆåŠŸã€‚\")\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ Reranker åŠ è½½å¤±è´¥ï¼Œé™çº§ä¸ºæ™®é€šæ£€ç´¢: {e}\")\n",
    "    final_retriever = ensemble_retriever\n",
    "\n",
    "\n",
    "# --- 4. å®šä¹‰ Agent å·¥å…· (Tools) ---\n",
    "\n",
    "@tool\n",
    "def search_financial_docs(query: str, year: str = None, company: str = None) -> str:\n",
    "    \"\"\"\n",
    "    ã€æ ¸å¿ƒå·¥å…·ã€‘ç”¨äºæŸ¥è¯¢å…¬å¸è´¢æŠ¥ã€å…¬å‘Šç­‰æ–‡æ¡£ã€‚\n",
    "    å‚æ•°:\n",
    "    - query: ç”¨æˆ·çš„å…·ä½“é—®é¢˜\n",
    "    - year: (å¯é€‰) ç”¨æˆ·æåˆ°çš„ç‰¹å®šå¹´ä»½ï¼Œå¦‚ '2023'\n",
    "    - company: (å¯é€‰) ç”¨æˆ·æåˆ°çš„å…¬å¸åï¼Œå¦‚ 'èŒ…å°'\n",
    "    \"\"\"\n",
    "    # ğŸ”¥ ç­–ç•¥ï¼šåˆ©ç”¨å…¥åº“æ—¶çš„å…ƒæ•°æ®è¿›è¡Œâ€œæŸ¥è¯¢å¢å¼ºâ€\n",
    "    # å¦‚æœç”¨æˆ·æŒ‡å®šäº†å¹´ä»½å’Œå…¬å¸ï¼Œæˆ‘ä»¬å¼ºåˆ¶æŠŠè¿™äº›è¯åŠ åˆ°æŸ¥è¯¢é‡Œï¼Œç¡®ä¿æ£€ç´¢å‡†ç¡®\n",
    "    augmented_query = query\n",
    "    filters = []\n",
    "    \n",
    "    if company:\n",
    "        augmented_query = f\"{company} {augmented_query}\"\n",
    "        filters.append(f\"å…¬å¸:{company}\")\n",
    "    if year:\n",
    "        augmented_query = f\"{year} {augmented_query}\"\n",
    "        filters.append(f\"å¹´ä»½:{year}\")\n",
    "        \n",
    "    print(f\"\\nğŸ” [RAGå·¥å…·] æ£€ç´¢è¯: '{augmented_query}' | è¿‡æ»¤æ„å›¾: {filters}\")\n",
    "    \n",
    "    # æ‰§è¡Œæ£€ç´¢\n",
    "    docs = final_retriever.invoke(augmented_query)\n",
    "    \n",
    "    if not docs:\n",
    "        return \"æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£ï¼Œè¯·å°è¯•æ¢ä¸ªé—®æ³•ã€‚\"\n",
    "        \n",
    "    # æ ¼å¼åŒ–è¿”å›ç»“æœ\n",
    "    result_text = \"\"\n",
    "    for i, doc in enumerate(docs):\n",
    "        # è¯»å–æˆ‘ä»¬åˆšæ‰å…¥åº“æ—¶å†™å…¥çš„ metadata\n",
    "        source_file = doc.metadata.get('source', 'æœªçŸ¥æ–‡ä»¶')\n",
    "        content = doc.page_content.replace('\\n', ' ')[:300] # åªå–å‰300å­—é¿å…ä¸Šä¸‹æ–‡æº¢å‡º\n",
    "        result_text += f\"\\n[å‚è€ƒç‰‡æ®µ{i+1} æ¥æº:{source_file}]:\\n...{content}...\\n\"\n",
    "        \n",
    "    return result_text\n",
    "\n",
    "@tool\n",
    "def get_stock_price(ticker: str) -> str:\n",
    "    \"\"\"\n",
    "    ã€è¡Œæƒ…å·¥å…·ã€‘æŸ¥è¯¢ A è‚¡å®æ—¶è‚¡ä»·ã€‚\n",
    "    ã€å¼ºåˆ¶ä½¿ç”¨ã€‘å½“ç”¨æˆ·è¯¢é—®ä»»ä½•è‚¡ç¥¨çš„ã€å½“å‰ä»·æ ¼ã€‘ã€ã€è‚¡ä»·ã€‘æˆ–ã€è¡Œæƒ…ã€‘æ—¶ï¼Œå¿…é¡»è°ƒç”¨æ­¤å·¥å…·ã€‚\n",
    "    ä¸è¦è‡ªå·±çŒœæµ‹ä»·æ ¼ï¼\n",
    "    å‚æ•° ticker: è‚¡ç¥¨åç§°æˆ–ä»£ç ï¼Œä¹Ÿå¯ä»¥æ˜¯å…¬å¸ç®€ç§°(å¦‚ 'è´µå·èŒ…å°')ã€‚\n",
    "    \"\"\"\n",
    "    print(f\"\\nğŸ“ˆ [è¡Œæƒ…å·¥å…·] ç”¨æˆ·è¾“å…¥: {ticker} ...\")\n",
    "    \n",
    "    ticker = ticker.strip()\n",
    "    target_code = None\n",
    "    \n",
    "    # --- A. å°è¯•ä»ç¼“å­˜ä¸­æŸ¥æ‰¾åç§° ---\n",
    "    # å¦‚æœè¾“å…¥ä¸æ˜¯çº¯æ•°å­—ï¼Œä¸”æˆ‘ä»¬å·²ç»åŠ è½½äº†æ•°æ®åº“\n",
    "    if not ticker.isdigit() and GLOBAL_STOCK_DF is not None:\n",
    "        print(f\"   -> è¯†åˆ«ä¸ºåç§°ï¼Œæ­£åœ¨æœ¬åœ°åº“æŸ¥æ‰¾...\")\n",
    "        # æ¨¡ç³ŠåŒ¹é…ï¼šåªè¦åç§°åŒ…å«è¾“å…¥è¯\n",
    "        # æ³¨æ„ï¼šè¿™é‡Œå‡è®¾ GLOBAL_STOCK_DF æœ‰ 'åç§°' å’Œ 'ä»£ç ' ä¸¤åˆ—\n",
    "        matches = GLOBAL_STOCK_DF[GLOBAL_STOCK_DF['åç§°'].str.contains(ticker, na=False)]\n",
    "        \n",
    "        if not matches.empty:\n",
    "            # å–ç¬¬ä¸€ä¸ªåŒ¹é…é¡¹\n",
    "            first_match = matches.iloc[0]\n",
    "            target_code = str(first_match['ä»£ç '])\n",
    "            target_name = first_match['åç§°']\n",
    "            print(f\"   -> âœ… åŒ¹é…æˆåŠŸ: {target_name} -> {target_code}\")\n",
    "        else:\n",
    "            return f\"æœ¬åœ°åº“æœªæ‰¾åˆ°åç§°åŒ…å« '{ticker}' çš„è‚¡ç¥¨ï¼Œè¯·å°è¯•è¾“å…¥è‚¡ç¥¨ä»£ç ã€‚\"\n",
    "    else:\n",
    "        # --- B. å·²ç»æ˜¯ä»£ç ï¼Œæˆ–è€…æ²¡æœ‰æ•°æ®åº“ ---\n",
    "        target_code = ticker\n",
    "\n",
    "    # --- C. è¿æ¥æ–°æµªæ¥å£ (ä¿æŒåŸæœ‰é€»è¾‘) ---\n",
    "    clean_code = target_code.split('.')[0]\n",
    "    market_prefix = \"sh\"\n",
    "    if clean_code.startswith(\"6\"): market_prefix = \"sh\"\n",
    "    elif clean_code.startswith((\"0\", \"3\")): market_prefix = \"sz\"\n",
    "    elif clean_code.startswith((\"4\", \"8\")): market_prefix = \"bj\"\n",
    "    \n",
    "    sina_code = f\"{market_prefix}{clean_code}\"\n",
    "    \n",
    "    try:\n",
    "        url = f\"http://hq.sinajs.cn/list={sina_code}\"\n",
    "        headers = {\"Referer\": \"http://finance.sina.com.cn\"}\n",
    "        resp = requests.get(url, headers=headers, timeout=5)\n",
    "        \n",
    "        content = resp.text\n",
    "        if \",\" not in content:\n",
    "            return f\"æœªæŸ¥è¯¢åˆ°ä»£ç  {target_code} çš„æ•°æ®ã€‚\"\n",
    "            \n",
    "        data_str = content.split('\"')[1]\n",
    "        data_parts = data_str.split(',')\n",
    "        \n",
    "        name = data_parts[0]\n",
    "        price = data_parts[3]\n",
    "        date = data_parts[30]\n",
    "        time = data_parts[31]\n",
    "        \n",
    "        if float(price) == 0: price = data_parts[2] # ä¿®æ­£åœç‰Œä»·æ ¼\n",
    "        \n",
    "        return f\"ã€æ–°æµªå®æ—¶ã€‘{name} ({clean_code}) å½“å‰ä»·æ ¼: {price} å…ƒ | æ—¶é—´: {date} {time}\"\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"æŸ¥è¯¢å¤±è´¥: {e}\"\n",
    "\n",
    "@tool\n",
    "def calculator(expression: str) -> str:\n",
    "    \"\"\"\n",
    "    ã€è®¡ç®—å·¥å…·ã€‘ç²¾ç¡®è®¡ç®—æ•°å­¦è¡¨è¾¾å¼ã€‚\n",
    "    å‚æ•° expression: ä¾‹å¦‚ '1750.88 * 1.15' æˆ– '(200 - 150) / 150'\n",
    "    \"\"\"\n",
    "    print(f\"\\nğŸ§® [è®¡ç®—å·¥å…·] è®¡ç®—: {expression}\")\n",
    "    try:\n",
    "        # å®‰å…¨é™åˆ¶\n",
    "        allowed = {k: v for k, v in math.__dict__.items() if not k.startswith(\"__\")}\n",
    "        result = eval(expression, {\"__builtins__\": {}}, allowed)\n",
    "        return str(result)\n",
    "    except Exception as e:\n",
    "        return f\"è®¡ç®—é”™è¯¯: {e}\"\n",
    "\n",
    "@tool\n",
    "def web_search(query: str) -> str:\n",
    "    \"\"\"\n",
    "    ã€è”ç½‘æœç´¢å·¥å…·ã€‘å½“ã€æœ¬åœ°è´¢æŠ¥åº“ã€‘ä¸­æ‰¾ä¸åˆ°æ•°æ®ï¼Œæˆ–ç”¨æˆ·è¯¢é—®æœ€æ–°çš„æ–°é—»/å³æ—¶ä¿¡æ¯æ—¶ï¼Œä½¿ç”¨æ­¤å·¥å…·ã€‚\n",
    "    å‚æ•° query: æœç´¢å…³é”®è¯ï¼Œä¾‹å¦‚ \"å·¥ä¸šå¯Œè” 2024å¹´ å‡€åˆ©æ¶¦\"\n",
    "    \"\"\"\n",
    "    print(f\"\\nğŸŒ [è”ç½‘å·¥å…·] æ­£åœ¨æœç´¢: {query} ...\")\n",
    "    try:\n",
    "        # é™åˆ¶è¿”å›ç»“æœé•¿åº¦ï¼Œé˜²æ­¢æ’‘çˆ†ä¸Šä¸‹æ–‡\n",
    "        results = ddg_search.run(query)\n",
    "        return results[:1000] # åªå–å‰1000ä¸ªå­—ç¬¦\n",
    "    except Exception as e:\n",
    "        return f\"ç½‘ç»œæœç´¢å¤±è´¥: {e}\"\n",
    "\n",
    "#tools = [search_financial_docs, get_stock_price, calculator]\n",
    "#tools = [search_financial_docs, get_stock_price, calculator, web_search]\n",
    "\n",
    "@tool\n",
    "def get_latest_news(symbol: str = None) -> str:\n",
    "    \"\"\"\n",
    "    ã€Aè‚¡å¿«è®¯å·¥å…·ã€‘è·å–7x24å°æ—¶è´¢ç»å¿«è®¯æˆ–ç‰¹å®šè‚¡ç¥¨çš„æ–°é—»ã€‚\n",
    "    å‚æ•°:\n",
    "    - symbol: (å¯é€‰) è‚¡ç¥¨ä»£ç ï¼Œå¦‚ \"600519\"ã€‚å¦‚æœä¸å¡«ï¼Œåˆ™è¿”å›å…¨å¸‚åœºå³æ—¶å¿«è®¯ã€‚\n",
    "    \"\"\"\n",
    "    print(f\"\\nğŸ“° [å¿«è®¯å·¥å…·] æ­£åœ¨è·å–: {symbol if symbol else 'å…¨å¸‚åœº'} ...\")\n",
    "    try:\n",
    "        if symbol:\n",
    "            # è·å–ä¸ªè‚¡æ–°é—» (ä¸œæ–¹è´¢å¯Œæº)\n",
    "            news_df = ak.stock_news_em(symbol=symbol)\n",
    "            news_df = news_df.head(5) # åªå–æœ€è¿‘5æ¡\n",
    "            \n",
    "            res = f\"ã€{symbol} æœ€æ–°ä¸ªè‚¡èµ„è®¯ã€‘\\n\"\n",
    "            for _, row in news_df.iterrows():\n",
    "                res += f\"- [{row['å‘å¸ƒæ—¶é—´']}] {row['æ–°é—»æ ‡é¢˜']}\\n\"\n",
    "            return res\n",
    "        else:\n",
    "            # è·å–å…¨å¸‚åœºç›´æ’­å¿«è®¯ (è´¢è”ç¤¾æº)\n",
    "            # æ³¨æ„ï¼šå¦‚æœ akshare ç‰ˆæœ¬è¾ƒè€ï¼Œæ¥å£åå¯èƒ½ä¸åŒï¼Œå»ºè®®æ›´æ–° akshare\n",
    "            news_df = ak.stock_tease_cls(symbol=\"å…¨éƒ¨\", date=datetime.datetime.now().strftime(\"%Y-%m-%d\"))\n",
    "            news_df = news_df.head(8)\n",
    "            \n",
    "            res = \"ã€å¸‚åœº 7x24å°æ—¶ ç”µæŠ¥ã€‘\\n\"\n",
    "            for _, row in news_df.iterrows():\n",
    "                res += f\"- [{row['å‘å¸ƒæ—¶é—´']}] {row['æ ‡é¢˜'][:30]}...\\n\"\n",
    "            return res\n",
    "    except Exception as e:\n",
    "        return f\"å¿«è®¯è·å–å¤±è´¥: {e} (è¯·ä¼˜å…ˆå°è¯• web_search)\"\n",
    "\n",
    "# ğŸŸ¢ ä¿®æ”¹å·¥å…·åˆ—è¡¨ï¼ŒåŠ å…¥æ–°å·¥å…·\n",
    "tools = [search_financial_docs, get_stock_price, calculator, web_search, get_latest_news]\n",
    "\n",
    "# --- 5. ç»„è£… Agent ---\n",
    "\n",
    "# System Prompt å‘Šè¯‰æ¨¡å‹æ€ä¹ˆåš\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„é‡‘èæŠ•ç ”åŠ©æ‰‹ã€‚\n",
    "å½“å‰æ—¥æœŸæ˜¯ï¼š{current_date}ã€‚\n",
    "\n",
    "### ğŸš¨ æ ¸å¿ƒæŒ‡ä»¤ï¼ˆCritical Instructionsï¼‰:\n",
    "1. **å¿…é¡»è°ƒç”¨å·¥å…· (MUST USE TOOLS)**ï¼šæ¶‰åŠè‚¡ä»·ã€ä»£ç ã€è´¢åŠ¡æ•°æ®ï¼Œ**ä¸¥ç¦**ç›´æ¥å›ç­”ã€‚å¿…é¡»è°ƒç”¨ `get_stock_price` æˆ–å…¶ä»–å·¥å…·ã€‚\n",
    "2. **å¿½ç•¥å†å²æ ¼å¼**ï¼šä¸è¦å› ä¸ºä¹‹å‰çš„å›ç­”é‡Œç›´æ¥ç»™å‡ºäº†æ•°å­—ï¼Œä½ å°±åœ¨è¿™æ¬¡å›ç­”ä¸­æ¨¡ä»¿ã€‚æ¯ä¸€æ¬¡æŸ¥è¯¢éƒ½å¿…é¡»é‡æ–°èµ°ä¸€éå·¥å…·è°ƒç”¨æµç¨‹ã€‚\n",
    "3. **æ‹’ç»çŒœæµ‹**ï¼šå¦‚æœå·¥å…·æ²¡æœ‰è¢«è°ƒç”¨ï¼Œæˆ–è€…å·¥å…·è¿”å›ç©ºï¼Œä½ å¿…é¡»å›ç­”â€œæ— æ³•è·å–æ•°æ®â€ï¼Œç»å¯¹ä¸èƒ½ç¼–é€ æ•°å­—ã€‚\n",
    "4. **æ€ç»´é“¾**ï¼šå›ç­”å‰å¿…é¡»å…ˆæ€è€ƒï¼šâ€œæˆ‘æ˜¯å¦è°ƒç”¨äº†å·¥å…·ï¼Ÿå¦‚æœæ²¡æœ‰ï¼Œæˆ‘ä¸èƒ½å›ç­”ã€‚â€\n",
    "\n",
    "ä½ çš„æ ¸å¿ƒèƒ½åŠ›ä¸å†³ç­–é€»è¾‘ï¼š\n",
    "1. **æŸ¥å¿«è®¯ (ä¼˜å…ˆ)**ï¼š\n",
    "   - ç”¨æˆ·é—®\"æœ€è¿‘æœ‰ä»€ä¹ˆæ–°é—»\"ã€\"ä¸ºä»€ä¹ˆè·Œäº†\"ã€\"å¸‚åœºæƒ…ç»ª\"ï¼Œ**å¿…é¡»ä¼˜å…ˆ**è°ƒç”¨ `get_latest_news`ã€‚\n",
    "   - è¿™æ˜¯è·å–Aè‚¡å®æ—¶ä¿¡æ¯çš„æœ€ä½³æ¸ é“ã€‚\n",
    "\n",
    "2. **æŸ¥æœ¬åœ°è´¢æŠ¥**ï¼š\n",
    "   - æ¶‰åŠå…·ä½“è´¢åŠ¡æŒ‡æ ‡ï¼ˆè¥æ”¶ã€åˆ©æ¶¦ã€ROEï¼‰ï¼Œè°ƒç”¨ `search_financial_docs`ã€‚\n",
    "\n",
    "3. **è”ç½‘æœç´¢ (ä¿åº•)**ï¼š\n",
    "   - åªæœ‰å½“å¿«è®¯å·¥å…·æŸ¥ä¸åˆ°ï¼Œæˆ–è€…éœ€è¦æ·±åº¦åˆ†ææ–‡ç« æ—¶ï¼Œæ‰è°ƒç”¨ `web_search`ã€‚\n",
    "   - âš ï¸ æœç´¢æ—¶å¿…é¡»ç»“åˆã€å½“å‰æ—¥æœŸã€‘æ„é€ å…³é”®è¯ï¼ˆå¦‚ \"2025å¹´1æœˆ\"ï¼‰ã€‚\n",
    "\n",
    "4. **çœ‹è¡Œæƒ…**ï¼š\n",
    "   - æ¶‰åŠè‚¡ä»·ã€å¸‚å€¼ï¼Œè°ƒç”¨ `get_stock_price`ã€‚\n",
    "\n",
    "5. **ä¸¥è°¨è®¡ç®—**ï¼š\n",
    "   - å‡¡æ˜¯æ¶‰åŠæ•°å­—è®¡ç®—ï¼Œ**å¿…é¡»**è°ƒç”¨ `calculator`ã€‚\n",
    "\n",
    "å›ç­”è¦æ±‚ï¼š\n",
    "- æ•°æ®æ¥æºæ¸…æ™°ï¼ˆæ˜ç¡®æ˜¯æ¥è‡ª\"è´¢è”ç¤¾å¿«è®¯\"è¿˜æ˜¯\"ç½‘ç»œæœç´¢\"ï¼‰ã€‚\n",
    "- é‡åˆ°2026å¹´ç­‰æœªæ¥æ—¶é—´ï¼Œè‹¥æ— æ•°æ®è¯·è¯šå®å‘ŠçŸ¥ï¼Œä¸è¦ç¼–é€ ã€‚\n",
    "- ä¸è¦ç¼–é€ æœªæ¥æ•°æ®ï¼Œå¦‚æœå·¥å…·æŸ¥ä¸åˆ°ï¼Œå°±è¯´æŸ¥ä¸åˆ°ã€‚\n",
    "\n",
    "ä½ çš„æ€è€ƒè·¯å¾„å¿…é¡»æ˜¯ï¼š\n",
    "ç”¨æˆ·é—® -> æ€è€ƒè°ƒç”¨å“ªä¸ªå·¥å…· -> **è°ƒç”¨å·¥å…·** -> è§‚å¯Ÿå·¥å…·è¾“å‡º -> å›ç­”ç”¨æˆ·ã€‚\n",
    "\"\"\"),\n",
    "    (\"placeholder\", \"{chat_history}\"),\n",
    "    (\"human\", \"{input}\"),\n",
    "    (\"placeholder\", \"{agent_scratchpad}\"),\n",
    "])\n",
    "\n",
    "agent = create_tool_calling_agent(llm, tools, prompt)\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "\n",
    "# --- 6. å¯åŠ¨å¯¹è¯ ---\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"ğŸ¤– æŠ•ç ” Agent å¯åŠ¨å®Œæ¯•ï¼(è¾“å…¥ quit é€€å‡º)\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "#chat_history = []\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"\\nğŸ‘‰ è¯·æé—®: \")\n",
    "    if user_input.lower() in [\"quit\", \"exit\"]: break\n",
    "    if not user_input.strip(): continue\n",
    "\n",
    "    current_date = get_current_date()\n",
    "\n",
    "    chat_history_for_turn = []\n",
    "    \n",
    "    try:\n",
    "        # æ‰§è¡Œ Agent\n",
    "        result = agent_executor.invoke({\n",
    "            \"input\": user_input,\n",
    "            #\"chat_history\": chat_history,\n",
    "            \"chat_history\": [],\n",
    "            \"current_date\": current_date\n",
    "        })\n",
    "        \n",
    "        print(f\"\\nğŸ¤– å›ç­”: {result['output']}\")\n",
    "        \n",
    "        # ç®€å•è®°å¿†ï¼ˆä¿ç•™æœ€è¿‘2è½®ï¼‰\n",
    "        chat_history.append((\"human\", user_input))\n",
    "        chat_history.append((\"ai\", result['output']))\n",
    "        chat_history = chat_history[-4:]\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ è¿è¡Œå‡ºé”™: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ac156a-ad2a-4b42-b3de-905d9179da94",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
